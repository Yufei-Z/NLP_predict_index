{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nb_black\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# os.chdir('D:\\Stock_Market_Sentiment_Analysis-master\\Stock_Market_Sentiment_Analysis-master')\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "K_Best_Features = 3000\n",
    "\n",
    "weights = {0:1, 1:3}\n",
    "def KFold_validation(clf, X, y):\n",
    "    acc = []\n",
    "    pos_precision, pos_recall, pos_f1_score = [], [], []\n",
    "    neg_precision, neg_recall, neg_f1_score = [], [], []\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train, test in kf.split(X):\n",
    "        X_train = [X[i] for i in train]\n",
    "        X_test = [X[i] for i in test]\n",
    "        y_train = [y[i] for i in train]\n",
    "        y_test = [y[i] for i in test]\n",
    "\n",
    "        # vectorizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x : (w for w in x.split(' ') if w.strip()))\n",
    "        def dummy_fun(doc):\n",
    "            return doc\n",
    "\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=dummy_fun,\n",
    "            preprocessor=dummy_fun,\n",
    "            token_pattern=None,\n",
    "        )\n",
    "\n",
    "        vectorizer.fit(X_train)\n",
    "        X_train = vectorizer.transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "\n",
    "        acc.append(metrics.accuracy_score(y_test, preds))\n",
    "        pos_precision.append(metrics.precision_score(y_test, preds, pos_label=1))\n",
    "        pos_recall.append(metrics.recall_score(y_test, preds, pos_label=1))\n",
    "        pos_f1_score.append(metrics.f1_score(y_test, preds, pos_label=1))\n",
    "        neg_precision.append(metrics.precision_score(y_test, preds, pos_label=0))\n",
    "        neg_recall.append(metrics.recall_score(y_test, preds, pos_label=0))\n",
    "        neg_f1_score.append(metrics.f1_score(y_test, preds, pos_label=0))\n",
    "\n",
    "    return (\n",
    "        np.mean(acc),\n",
    "        np.mean(pos_precision),\n",
    "        np.mean(pos_recall),\n",
    "        np.mean(pos_f1_score),\n",
    "        np.mean(neg_precision),\n",
    "        np.mean(neg_recall),\n",
    "        np.mean(neg_f1_score),\n",
    "    )\n",
    "\n",
    "\n",
    "def benchmark_clfs(X, y):\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    classifiers = [\n",
    "        (\"LinearSVC\", svm.LinearSVC()),\n",
    "        (\"LogisticReg\", LogisticRegression()),\n",
    "        (\"SGD\", SGDClassifier()),\n",
    "        (\"MultinomialNB\", naive_bayes.MultinomialNB()),\n",
    "        (\"KNN\", KNeighborsClassifier()),\n",
    "        (\"DecisionTree\", DecisionTreeClassifier()),\n",
    "        (\"RandomForest\", RandomForestClassifier()),\n",
    "        (\"AdaBoost\", AdaBoostClassifier(base_estimator=LogisticRegression())),\n",
    "    ]\n",
    "\n",
    "    cols = [\n",
    "        \"metrics\",\n",
    "        \"accuracy\",\n",
    "        \"pos_precision\",\n",
    "        \"pos_recall\",\n",
    "        \"pos_f1_score\",\n",
    "        \"neg_precision\",\n",
    "        \"neg_recall\",\n",
    "        \"neg_f1_score\",\n",
    "    ]\n",
    "    scores = []\n",
    "    for name, clf in classifiers:\n",
    "        score = KFold_validation(clf, X, y)\n",
    "        row = [name]\n",
    "        row.extend(score)\n",
    "        scores.append(row)\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=cols).T\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[[0]], inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "\n",
    "def eval_model(X, y):\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer=\"word\", tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None\n",
    "    )\n",
    "\n",
    "    X = vectorizer.fit_transform(X)\n",
    "\n",
    "    print(\"Train model...\")\n",
    "    clf.fit(X, y,class_weights = {0:1, 1:3})\n",
    "\n",
    "    print(\"Loading comments...\")\n",
    "    df = pd.read_csv(comment_file)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df[\"created_time\"] = pd.to_datetime(df[\"created_time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"polarity\"] = 0\n",
    "    df[\"title\"].apply(lambda x: [w.strip() for w in x.split()])\n",
    "\n",
    "    texts = df[\"title\"]\n",
    "    texts = vectorizer.transform(texts)\n",
    "\n",
    "    preds = clf.predict(texts)\n",
    "    df[\"polarity\"] = preds\n",
    "\n",
    "    df.to_csv(\"stock_comments_analyzed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "\n",
    "pro = ts.pro_api()\n",
    "\n",
    "period = 1\n",
    "id = \"000002\"\n",
    "\n",
    "\n",
    "if int(id) > 100000:\n",
    "    stock_code = id + \".SH\"\n",
    "else:\n",
    "    stock_code = id + \".SZ\"\n",
    "quotes = pro.daily(ts_code=stock_code, start_date=\"20080101\")\n",
    "quotes.set_index(\"trade_date\", inplace=True)\n",
    "pct_chg = quotes[\"pct_chg\"].shift(periods=period, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def dummy_fun(doc):\n",
    "            return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "            analyzer=\"word\",\n",
    "            tokenizer=dummy_fun,\n",
    "            preprocessor=dummy_fun,\n",
    "            token_pattern=None,\n",
    "        )\n",
    "thre = np.mean(merged[\"pct_chg\"]) + 0.5*np.std(merged[\"pct_chg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged[\"title\"]\n",
    "y = merged[\"pct_chg\"] > thre\n",
    "X_train = [X[i] for i in range(290000,310000)]#X[290000:310000]\n",
    "y_train =[1*y[i] for i in range(290000,310000)]\n",
    "X_test = [X[i] for i in range(310000,324832)]\n",
    "vectorizer.fit(X_train)\n",
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \"\n",
    ")\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = True\n",
    "if word_embedding:\n",
    "    print(\"Embedding...\")\n",
    "    EMBEDDING_FILE = \"D:/sgns.financial.bigram-char\"\n",
    "    embed_size = 300\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype=\"float32\")\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        get_coefs(*o.rstrip().rsplit(\" \"))\n",
    "        for o in open(EMBEDDING_FILE, encoding=\"ISO-8859-1\")\n",
    "    )\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)\n",
    "max_len = 20\n",
    "X_train=tokenizer.texts_to_sequences(X[315000:320000])\n",
    "X_test = tokenizer.texts_to_sequences(X[320000:])\n",
    "X_train_padded_seqs = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test_padded_seqs = pad_sequences(X_test, maxlen=max_len)\n",
    "x_train = tokenizer.sequences_to_matrix(X_train_padded_seqs, mode=\"binary\")\n",
    "x_test = tokenizer.sequences_to_matrix(X_test_padded_seqs, mode=\"binary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_matrix(X[315000:315001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_matrix(X_train[0:2])[1][357815]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "        clf = KNeighborsClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences(X[315000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[315000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=merged.iloc[range(310000,324832)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = m['p'].groupby(m['created_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BI_Simple_func(row):\n",
    "    pos = row[row == 1].count()\n",
    "    neg = row[row == 0].count()\n",
    "\n",
    "    return (pos-neg)/(pos+neg)\n",
    "\n",
    "BI_Simple_index = mg.apply(BI_Simple_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(BI_Simple_index)[50:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=BI_Simple_index>-0.8\n",
    "d.index = pd.to_datetime(d.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(BI_Simple_index>-0.7)\n",
    "M  = pd.merge(d, pct_chg, how=\"left\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M['profit'] = M['p']*M['pct_chg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = M['profit'].iloc[0:217]#remove last 2 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(np.cumproduct(1+profit/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(profit)/np.std(profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumproduct(1+profit/100))\n",
    "plt.plot(M['pct_chg'].iloc[0:217])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(profit==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"D:/股吧评论/gu000002.xlsx\")\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"created_time\"] = pd.to_datetime(\n",
    "    df[\"Date\"], format=\"%Y-%m-%d %H:%M\", errors=\"coerce\"\n",
    ")\n",
    "df[\"created_time\"] = df[\"created_time\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df = df[df[\"title\"] != 0]\n",
    "df.index = pd.to_datetime(df[\"created_time\"])\n",
    "merged = pd.merge(df, pct_chg, how=\"right\", left_index=True, right_index=True)\n",
    "merged = merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "thre = np.mean(merged[\"pct_chg\"]) + 0.5*np.std(merged[\"pct_chg\"])\n",
    "\n",
    "X = merged[\"title\"]\n",
    "y = merged[\"pct_chg\"] > thre\n",
    "X = X[290000:]\n",
    "y = y[290000:]\n",
    "scores = benchmark_clfs(X, y)\n",
    "print(scores)\n",
    "scores.to_csv(\"model_ml_scores1.csv\", float_format=\"%.4f\")\n",
    "\n",
    "\n",
    "# eval_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dense, Activation, Input\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = np.mean(merged[\"pct_chg\"]) + 0.5*np.std(merged[\"pct_chg\"])\n",
    "\n",
    "weights = {0:1, 1:3}\n",
    "X = merged[\"title\"]\n",
    "y = merged[\"pct_chg\"] > thre\n",
    "X = X[310000:]\n",
    "y = y[310000:]\n",
    "tokenizer = Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \"\n",
    ")\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab = tokenizer.word_index\n",
    "print(\"Vocab size\", len(vocab))\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "max_len = 64\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_train_word_ids = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded_seqs = pad_sequences(X_train_word_ids, maxlen=max_len)\n",
    "X_test_padded_seqs = pad_sequences(X_test_word_ids, maxlen=max_len)\n",
    "\n",
    "word_embedding = True\n",
    "if word_embedding:\n",
    "    print(\"Embedding...\")\n",
    "    EMBEDDING_FILE = \"D:/sgns.financial.bigram-char\"\n",
    "    embed_size = 300\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype=\"float32\")\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        get_coefs(*o.rstrip().rsplit(\" \"))\n",
    "        for o in open(EMBEDDING_FILE, encoding=\"ISO-8859-1\")\n",
    "    )\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        #        print(\" — val_f1: % f — val_precision: % f — val_recall % f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "\n",
    "def train_model_MLP():\n",
    "    x_train = tokenizer.sequences_to_matrix(X_train_word_ids, mode=\"binary\")\n",
    "    x_test = tokenizer.sequences_to_matrix(X_test_word_ids, mode=\"binary\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(vocab) + 1,), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    model_checkpoint = ModelCheckpoint(\"./model-MLP.h5\", save_best_only=True)\n",
    "    metrics = Metrics()\n",
    "    hist = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=40,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[metrics, early_stopping, model_checkpoint],\n",
    "        class_weight=weights,\n",
    "    )\n",
    "\n",
    "    best_acc = max(hist.history[\"val_accuracy\"])\n",
    "    idx = np.argmax(hist.history[\"val_accuracy\"])\n",
    "    posi_precision = metrics.val_precisions[idx]\n",
    "    \n",
    "    recall = metrics.val_recalls[idx]\n",
    "    f1score = metrics.val_f1s[idx]\n",
    "\n",
    "    del model, early_stopping, model_checkpoint, metrics\n",
    "    gc.collect()\n",
    "\n",
    "    return (best_acc, precision, recall, f1score)\n",
    "\n",
    "\n",
    "def train_model_LSTM():\n",
    "    model = Sequential()\n",
    "    embed_size = 300\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            len(vocab) + 1,\n",
    "            embed_size,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=True,\n",
    "        )\n",
    "    )\n",
    "    #    model.add(Embedding(len(vocab)+1, embed_size, input_length=max_len))\n",
    "    model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.2))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    model_checkpoint = ModelCheckpoint(\"model-LSTM.h5\", save_best_only=True)\n",
    "    metrics = Metrics()\n",
    "    hist = model.fit(\n",
    "        X_train_padded_seqs,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=100,\n",
    "        validation_data=(X_test_padded_seqs, y_test),\n",
    "        callbacks=[metrics, early_stopping, model_checkpoint],\n",
    "        class_weight=weights,\n",
    "    )\n",
    "\n",
    "    best_acc = max(hist.history[\"val_accuracy\"])\n",
    "    idx = np.argmax(hist.history[\"val_accuracy\"])\n",
    "    precision = metrics.val_precisions[idx]\n",
    "    recall = metrics.val_recalls[idx]\n",
    "    f1score = metrics.val_f1s[idx]\n",
    "\n",
    "    del model, early_stopping, model_checkpoint, metrics\n",
    "    gc.collect()\n",
    "\n",
    "    return (best_acc, precision, recall, f1score)\n",
    "\n",
    "\n",
    "def train_model_TextCNN():\n",
    "    main_input = Input(shape=(max_len,), dtype=\"float64\")\n",
    "    embed_size = 300\n",
    "    embedder = Embedding(len(vocab) + 1, embed_size, input_length=max_len)\n",
    "    embed = embedder(main_input)\n",
    "    cnn1 = Convolution1D(256, 3, padding=\"same\", strides=1, activation=\"relu\")(embed)\n",
    "    cnn1 = MaxPool1D(pool_size=4)(cnn1)\n",
    "    cnn2 = Convolution1D(256, 4, padding=\"same\", strides=1, activation=\"relu\")(embed)\n",
    "    cnn2 = MaxPool1D(pool_size=4)(cnn2)\n",
    "    cnn3 = Convolution1D(256, 5, padding=\"same\", strides=1, activation=\"relu\")(embed)\n",
    "    cnn3 = MaxPool1D(pool_size=4)(cnn3)\n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(1, activation=\"sigmoid\")(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    model_checkpoint = ModelCheckpoint(\"model-TextCNN.h5\", save_best_only=True)\n",
    "    metrics = Metrics()\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_train_padded_seqs,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        validation_data=(X_test_padded_seqs, y_test),\n",
    "        callbacks=[early_stopping, metrics, model_checkpoint],\n",
    "        class_weight=weights,\n",
    "    )\n",
    "\n",
    "    best_acc = max(hist.history[\"val_accuracy\"])\n",
    "    idx = np.argmax(hist.history[\"val_accuracy\"])\n",
    "    precision = metrics.val_precisions[idx]\n",
    "    recall = metrics.val_recalls[idx]\n",
    "    f1score = metrics.val_f1s[idx]\n",
    "\n",
    "    del model, early_stopping, model_checkpoint, metrics\n",
    "    gc.collect()\n",
    "\n",
    "    return (best_acc, precision, recall, f1score)\n",
    "\n",
    "\n",
    "def eval_models():\n",
    "    scores = []\n",
    "\n",
    "    score = [\"NN(MLP)\"]\n",
    "    score.extend(train_model_MLP())\n",
    "    scores.append(score)\n",
    "\n",
    "    score = [\"CNN(TextCNN)\"]\n",
    "    score.extend(train_model_TextCNN())\n",
    "    scores.append(score)\n",
    "\n",
    "    score = [\"RNN(LSTM)\"]\n",
    "    score.extend(train_model_LSTM())\n",
    "    scores.append(score)\n",
    "\n",
    "    df = pd.DataFrame(scores).T\n",
    "    df.index = [\"model\", \"accuracy\", \"precision\", \"recall\", \"f1score\"]\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[[0]], inplace=True)\n",
    "    df = df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = eval_models()\n",
    "\n",
    "    df.to_csv(\"model_dl_scoresq.csv\", float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = np.mean(merged[\"pct_chg\"]) + 0.5*np.std(merged[\"pct_chg\"])\n",
    "\n",
    "weights = {0:1, 1:3}\n",
    "X = merged[\"title\"]\n",
    "y = merged[\"pct_chg\"] > thre\n",
    "X = X[310000:]\n",
    "y = y[310000:]\n",
    "tokenizer = Tokenizer(\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \"\n",
    ")\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab = tokenizer.word_index\n",
    "print(\"Vocab size\", len(vocab))\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "max_len = 64\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_train_word_ids = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_padded_seqs = pad_sequences(X_train_word_ids, maxlen=max_len)\n",
    "X_test_padded_seqs = pad_sequences(X_test_word_ids, maxlen=max_len)\n",
    "\n",
    "word_embedding = True\n",
    "if word_embedding:\n",
    "    print(\"Embedding...\")\n",
    "    EMBEDDING_FILE = \"D:/sgns.financial.bigram-char\"\n",
    "    embed_size = 300\n",
    "\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype=\"float32\")\n",
    "\n",
    "    embeddings_index = dict(\n",
    "        get_coefs(*o.rstrip().rsplit(\" \"))\n",
    "        for o in open(EMBEDDING_FILE, encoding=\"ISO-8859-1\")\n",
    "    )\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        #        print(\" — val_f1: % f — val_precision: % f — val_recall % f\" % (_val_f1, _val_precision, _val_recall))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    x_train = tokenizer.sequences_to_matrix(X_train_word_ids, mode=\"binary\")\n",
    "    x_test = tokenizer.sequences_to_matrix(X_test_word_ids, mode=\"binary\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(vocab) + 1,), activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "    model_checkpoint = ModelCheckpoint(\"./model-MLP.h5\", save_best_only=True)\n",
    "    metrics = Metrics()\n",
    "    hist = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=40,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[metrics, early_stopping, model_checkpoint],\n",
    "        class_weight=weights,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    best_acc = max(hist.history[\"val_accuracy\"])\n",
    "    idx = np.argmax(hist.history[\"val_accuracy\"])\n",
    "    precision = metrics.val_precisions[idx]\n",
    "    recall = metrics.val_recalls[idx]\n",
    "    f1score = metrics.val_f1s[idx]\n",
    "    print([precision,recall,f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_train).shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[1:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['y']=merged['pct_chg']>thre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf=merged[['title','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.iloc[0:200000].to_csv('train.txt',sep='\\t',index=False,header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.iloc[220000:].to_csv('test.txt',sep='\\t',index=False,header=0)\n",
    "ndf.iloc[200000:220000].to_csv('val.txt',sep='\\t',index=False,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.iloc[150000:200000].to_csv('train2.txt',sep='\\t',index=False,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.iloc[310000:].to_csv('test2.txt',sep='\\t',index=False,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
